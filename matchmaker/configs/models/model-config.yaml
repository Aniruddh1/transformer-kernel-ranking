#
# Model / training config with all non-path or collection params
# --------------------------------------------------------------

#
# meta stuff
#

# -1 for disabling this feature, and only run validation after every epoch
validate_every_n_batches: 2000
validation_cont_use_cache: True

token_embedder_type: "embedding" # embedding,fasttext,bert_cls
train_embedding: True
sparse_gradient_embedding: True

use_fp16: False

random_seed: 208973249 # real-random (from random.org)

# needs to be the same model + model_config + token_embedder_type + vocabulary!
# ATTENTION !! this makes reproducability awfully difficult 
#warmstart_model_path: ""

#
# Models
# -------------------------------------------------------
# * Duet + Pacrr need a min_doc/query_length set
#
# * bert_cls also needs the token_embedder_type=bert_cls, as this will shake things up quite a bit 
#

model: "TKL"

#
# optimization
#

validation_metric: "MRR@10"

optimizer: "adam"

# default group (all params are in here if not otherwise specified in param_group1_names)
param_group0_learning_rate: 0.0001
param_group0_weight_decay: 0

param_group1_names: ["dense","position_bias","position_bias_absolute"] 
param_group1_learning_rate: 0.001
param_group1_weight_decay: 0

embedding_optimizer: "sparse_adam"
embedding_optimizer_learning_rate: 0.0001
embedding_optimizer_momentum: 0.8 # only when using sgd

# disable with factor = 1 
learning_rate_scheduler_patience: 10 # * validate_every_n_batches = batch count to check
learning_rate_scheduler_factor: 0.5

#
# train loop settings
#
epochs: 3
batch_size_train: 32
batch_size_eval: 256

gradient_accumulation_steps: -1

early_stopping_patience: 35 # * validate_every_n_batches = batch count to check

#
# data loading settings
#

# max sequence lengths, disable cutting off with -1
max_doc_length: 2000
max_query_length: 30

# min sequence lengths, disable cutting off with -1 , some models (paccr,duet) need this 
#min_doc_length: 200
min_doc_length: -1
#min_query_length: 30
min_query_length: -1

#
# interpretability / secondary output 
#

secondary_output:
  top_n: 20

#
# per model params: specify with modelname_param: ...
# ----------------------------------------------------
#

tk_att_heads: 10
tk_att_layer: 2
tk_att_proj_dim: 32
tk_att_ff_dim: 100

tk_kernels_mu: [1.0, 0.9, 0.7, 0.5, 0.3, 0.1, -0.1, -0.3, -0.5, -0.7, -0.9]
tk_kernels_sigma: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]

# tk

tk_use_diff_posencoding: True

knrm_kernels: 11

conv_knrm_ngrams: 3
conv_knrm_kernels: 11
conv_knrm_conv_out_dim: 128 # F in the paper 

match_pyramid_conv_output_size : [16,16,16,16,16] 
match_pyramid_conv_kernel_size : [[3,3],[3,3],[3,3],[3,3],[3,3]]
match_pyramid_adaptive_pooling_size: [[36,90],[18,60],[9,30],[6,20],[3,10]]

mv_lstm_hidden_dim: 32
mv_top_k: 10

pacrr_unified_query_length: 30
pacrr_unified_document_length: 200
pacrr_max_conv_kernel_size: 3
pacrr_conv_output_size: 32
pacrr_kmax_pooling_size: 5

salc_conv_knrm_kernels: 11
salc_conv_knrm_conv_out_dim: 128
salc_conv_knrm_dropi: 0 
salc_conv_knrm_drops: 0
salc_conv_knrm_salc_dim: 300

salc_knrm_kernels: 11
salc_knrm_dropi: 0
salc_knrm_drops: 0
salc_knrm_salc_dim: 300

mm_light_kernels: 11