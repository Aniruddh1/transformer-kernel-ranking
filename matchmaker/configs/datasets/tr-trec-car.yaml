#
# train.py configuration file
# ----------------------------
#

expirement_base_path: "/data01/hofstaetter/trec-car-experiments"
tqdm_disabled: False


#
# training paths
#
train_tsv: "/data01/hofstaetter/data/trec-car/train/fold_0_train_top1000-split4/*"

#
# continuous validation path
#

validation_cont:
  tsv: "/data01/hofstaetter/data/trec-car/validation/fold4_top100-split6/*"
  qrels: "/data01/hofstaetter/data/trec-car/qrels/fold_4_full.qrels"
  candidate_set_from_to: [5,100]
  candidate_set_path: "/data01/hofstaetter/data/trec-car/fs_results/fold_4_validation_queries_bm25_default_top100.txt"
  save_only_best: True

#
# one time at the end validation (disable by commenting it out)
#

validation_end:
  top1000:
    tsv: "/data01/hofstaetter/data/trec-car/validation/fold4_top1000-split6/*"
    qrels: "/data01/hofstaetter/data/trec-car/qrels/fold_4_full.qrels"
    candidate_set_from_to: [1,1000]
    candidate_set_path: "/data01/hofstaetter/data/trec-car/fs_results/fold_4_validation_queries_bm25_default.txt"
    save_secondary_output: False


#
# test paths (names & datasets must match up with validation end -> for correct use of fixed cs@n)
#
test:
  top1000:
    tsv: "/data01/hofstaetter/data/trec-car/test/bY1_test_top1000-split6/*"
    qrels: "/data01/hofstaetter/data/trec-car/qrels/bY1_hierarchical.qrels"
    candidate_set_max: 1000
    candidate_set_path: "/data01/hofstaetter/data/trec-car/fs_results/benchmarkY1test.txt"
    save_secondary_output: True

#
# leaderboard (private eval set without qrels)
#
#leaderboard:
# empty:
#  tsv: "/data01/hofstaetter/data/msmarco/bm25best/eval.subset.top1000.tokenized-split6/*"
#  save_secondary_output: True

#
# pre-trained word representation inputs (embedding layer)
# --------------------------------------------------------
#

#
# token_embedder_type = embedding 
#
#pre_trained_embedding: "/data01/hofstaetter/data/glove/glove.42B.300d.txt"
pre_trained_embedding: "/dev/shm/sebastians_ram/glove.42B.300d-wheader.txt"

pre_trained_embedding_dim: 300
vocab_directory: "/data01/hofstaetter/data/trec-car/vocabs/lower_5"

#
# token_embedder_type =fasttext
#
fasttext_vocab_mapping: "/data01/hofstaetter/data/trec-car/fasttext/wiki_15-sg-1.5m-dim200-trigram-voc_mapping.txt"
fasttext_weights: "/data01/hofstaetter/data/trec-car/fasttext/wiki_15-sg-1.5m-dim200-trigram-weights.npy"
fasttext_max_subwords: 40
fasttext_merge_mode: "mean" # or sum

#
# token_embedder_type = bert_cls 
#
bert_pretrained_model: "bert-large-uncased" # or bert-large-uncased,bert-large-cased,bert-base-cased
bert_trainable: False

#
# pre-computed idf path
#
idf_path: "/data01/hofstaetter/data/trec-car/idf/vocab_full_log_idf.txt" # will be loaded only for models using it (set in the train.py model selection block)
idf_path_fasttext: "/data01/hofstaetter/data/trec-car/idf/vocab_full_log_idf_fasttext.npy" # if token_embedder_type: "fasttext"
idf_trainable: False