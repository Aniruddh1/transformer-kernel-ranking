#
# train.py configuration file
# ----------------------------
#

expirement_base_path: "/data01/hofstaetter/msmarco-document-experiments"
tqdm_disabled: False


#
# training paths
#
train_tsv: "/data01/hofstaetter/data/msmarco-document/train/msmarco-doctrain-top100-split6/*"
#train_tsv: "/data01/hofstaetter/data/msmarco-document/train/doctrain-top100-max-800-split6/*"
#train_tsv: "/data01/hofstaetter/data/msmarco-passage/train/triples.train.small-split4/*"

#
# continuous validation path
#

validation_cont:
  tsv: "/data01/hofstaetter/data/msmarco-document/validation/validation-queries_bm25plain_top100-split6/*"
  qrels: "/data01/hofstaetter/data/msmarco-document/qrels/msmarco-doctrain-qrels.tsv"
  candidate_set_from_to: [5,100]
  candidate_set_path: "/data01/hofstaetter/data/msmarco-document/fs_results/validation-queries.plain_index.bm25best.top100.txt"
  save_only_best: False

#
# one time at the end validation (disable by commenting it out)
#
validation_end:
  top1000:
    tsv: "/data01/hofstaetter/data/msmarco-document/validation/validation-queries_bm25plain_top1000-split6/*"
    qrels: "/data01/hofstaetter/data/msmarco-document/qrels/msmarco-doctrain-qrels.tsv"
    candidate_set_from_to: [1,1000]
    candidate_set_path: "/data01/hofstaetter/data/msmarco-document/fs_results/validation-queries.plain_index.bm25best.top1000.txt"
    save_secondary_output: False

#
# test paths
#
test:
  top1000:
    tsv: "/data01/hofstaetter/data/msmarco-document/test/docdev-queries_bm25plain_top1000-split6/*"
    qrels: "/data01/hofstaetter/data/msmarco-document/qrels/msmarco-docdev-qrels.tsv"
    candidate_set_max: 1000
    candidate_set_path: "/data01/hofstaetter/data/msmarco-document/fs_results/dev-queries.plain_index.bm25best.top1000.txt"
    save_secondary_output: True
#
# leaderboard (private eval set without qrels)
#

# official trec 2019 test queries
leaderboard:
  2019_full_rank:
    tsv: "/data01/hofstaetter/data/msmarco-document/leaderboard/test2019_bm25plain_top1000-split6/*"
    save_secondary_output: True
  2019_rerank_100:
    tsv: "/data01/hofstaetter/data/msmarco-document/leaderboard/msmarco-doctest2019-top100-split6/*"
    save_secondary_output: True

#
# pre-trained word representation inputs (embedding layer)
# --------------------------------------------------------
#

#
# token_embedder_type = embedding 
#pre_trained_embedding: "/data01/hofstaetter/data/glove/glove.42B.300d.txt"
pre_trained_embedding: "/dev/shm/sabastians_ram/glove.42B.300d-wheader.txt"
pre_trained_embedding_dim: 300
vocab_directory: "/data01/hofstaetter/data/msmarco-document/vocabs/lower_10"

#
# token_embedder_type =fasttext
#
fasttext_vocab_mapping: "/data01/hofstaetter/data/msmarco-document/fasttext/wiki200trigram_vocabfull.txt"
fasttext_weights: "/data01/hofstaetter/data/msmarco-document/fasttext/wiki200trigram_weights.npy"
fasttext_max_subwords: 60
fasttext_merge_mode: "sum" # or sum

#
# token_embedder_type = bert_cls 
#
bert_pretrained_model: "bert-base-uncased" # or bert-large-uncased,bert-large-cased,bert-base-cased
bert_trainable: False

#
# pre-computed idf path
#
idf_path: "/data01/hofstaetter/data/msmarco-document/idf/vocab_full_log_idf.txt" # will be loaded only for models using it (set in the train.py model selection block)
idf_path_fasttext: "/data01/hofstaetter/data/msmarco-document/idf/vocab_full_log_idf_fasttext.npy" # if token_embedder_type: "fasttext"
idf_trainable: False